> ğŸ“ Click on the language section to expand / è¨€èªã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å±•é–‹

# Kandinsky 5

## Overview / æ¦‚è¦

This is an unofficial training and inference script for [Kandinsky 5](https://github.com/ai-forever/Kandinsky-5). The features are as follows:

- fp8 support and memory reduction by block swap
- Inference without installing Flash attention (using PyTorch's scaled dot product attention)
- LoRA training for text-to-video (T2V), image-to-video (I2V, Pro) models, and Image (T2I, Edit) models

This feature is experimental.

<details>
<summary>æ—¥æœ¬èª</summary>

[Kandinsky 5](https://github.com/ai-forever/Kandinsky-5) ã®éå…¬å¼ã®å­¦ç¿’ãŠã‚ˆã³æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚

ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ï¼š

- fp8å¯¾å¿œãŠã‚ˆã³block swapã«ã‚ˆã‚‹çœãƒ¡ãƒ¢ãƒªåŒ–
- Flash attentionã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãªã—ã§ã®å®Ÿè¡Œï¼ˆPyTorchã®scaled dot product attentionã‚’ä½¿ç”¨ï¼‰
- ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ“ãƒ‡ã‚ªã¸ã®å¤‰æ› (T2V)ã€ç”»åƒã‹ã‚‰ãƒ“ãƒ‡ã‚ªã¸ã®å¤‰æ› (I2Vã€Pro) ãƒ¢ãƒ‡ãƒ«ã€ãŠã‚ˆã³ç”»åƒ (T2Iã€Edit) ãƒ¢ãƒ‡ãƒ«ã® LoRA ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

ã“ã®æ©Ÿèƒ½ã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚

</details>

## Download the model / ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

Download the model weights from the [Kandinsky 5.0 Collection](https://huggingface.co/collections/ai-forever/kandinsky-50) on Hugging Face.

### DiT Model / DiTãƒ¢ãƒ‡ãƒ«

This document focuses on **Pro** models. The trainer also works with **Lite** models.
æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ **Pro** ãƒ¢ãƒ‡ãƒ«ã‚’ä¸­å¿ƒã«èª¬æ˜ã—ã¾ã™ãŒã€ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã¯ **Lite** ãƒ¢ãƒ‡ãƒ«ã§ã‚‚å‹•ä½œã—ã¾ã™ã€‚

Download a Pro DiT `.safetensors` checkpoint from the Kandinsky 5.0 Collection (e.g. `kandinsky5pro_t2v_pretrain_5s.safetensors` or `kandinsky5pro_i2v_sft_5s.safetensors`).

### VAE

Kandinsky 5 uses the HunyuanVideo 3D VAE for video tasks. Download `diffusion_pytorch_model.safetensors` (or `pytorch_model.pt`) from:
https://huggingface.co/hunyuanvideo-community/HunyuanVideo . Image generation/edit tasks use [Flux 1 VAE](https://huggingface.co/black-forest-labs/FLUX.1-dev/tree/main/vae)

### Text Encoders / ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€

Kandinsky 5 uses Qwen2.5-VL-7B and CLIP for text encoding.

**Qwen2.5-VL-7B**: Download from https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct (or use the path to your local Qwen/Qwen2.5-VL-7B-Instruct model)

**CLIP**: Use the Hugging Face Transformers model `openai/clip-vit-large-patch14`.

Pass either the model ID (e.g., `--text_encoder_clip openai/clip-vit-large-patch14`) or a path to the locally cached snapshot directory.

### Directory Structure / ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

Place them in your chosen directory structure:

```
weights/
â”œâ”€â”€ model/
â”‚   â””â”€â”€ kandinsky5pro_t2v_pretrain_5s.safetensors
â”œâ”€â”€ vae/
â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors
â”œâ”€â”€ text_encoder/
â”‚   â””â”€â”€ (Qwen2.5-VL-7B files)
â””â”€â”€ text_encoder2/
    â””â”€â”€ (openai/clip-vit-large-patch14 files)
```

<details>
<summary>æ—¥æœ¬èª</summary>

Hugging Faceã®[Kandinsky 5.0 Collection](https://huggingface.co/collections/ai-forever/kandinsky-50)ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ **Proãƒ¢ãƒ‡ãƒ«** ã‚’å‰æã«èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚

**DiTãƒ¢ãƒ‡ãƒ«**: ä¸Šè¨˜ã®ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰`.safetensors`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚

**VAE**: Kandinsky 5 ã¯ã€ãƒ“ãƒ‡ã‚ª ã‚¿ã‚¹ã‚¯ã« HunyuanVideo 3D VAE ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ä»¥ä¸‹ã‹ã‚‰ `diffusion_pytorch_model.safetensors` (ã¾ãŸã¯ `pytorch_model.pt`) ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
https://huggingface.co/hunyuanvideo-community/HunyuanVideo ã€‚ç”»åƒç”Ÿæˆ/ç·¨é›†ã‚¿ã‚¹ã‚¯ã§ã¯[Flux 1 VAE](https://huggingface.co/black-forest-labs/FLUX.1-dev/tree/main/vae)ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

**ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€**: Qwen2.5-VL-7Bã¨CLIPã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

**Qwen2.5-VL-7B**: https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼ˆã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ã® `Qwen/Qwen2.5-VL-7B-Instruct` ã‚’æŒ‡å®šã—ã¾ã™ï¼‰ã€‚

**CLIP**: Hugging Face Transformersã® `openai/clip-vit-large-patch14` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼ˆãƒ¢ãƒ‡ãƒ«IDã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸsnapshotãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¾ã™ï¼‰ã€‚

ä»»æ„ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚

</details>

## List of Kandinsky 5 models / åˆ©ç”¨å¯èƒ½ãªã‚¿ã‚¹ã‚¯

The `--task` option selects a model configuration (architecture, attention type, resolution, and default parameters).
The DiT checkpoint must be set explicitly via `--dit` (this overrides the task's default checkpoint path).

| # | Task | Checkpoint | Parameters | HF URL |
|---|---|---|---|---|
| 1 | k5-pro-t2v-5s-sd | kandinsky5pro_t2v_sft_5s.safetensors | T2V, 5s, 19B, Pro SFT | [kandinskylab/Kandinsky-5.0-T2V-Pro-sft-5s](https://huggingface.co/kandinskylab/Kandinsky-5.0-T2V-Pro-sft-5s) |
| 2 | k5-pro-t2v-10s-sd | kandinsky5pro_t2v_sft_10s.safetensors | T2V, 10s, 19B, Pro SFT | [kandinskylab/Kandinsky-5.0-T2V-Pro-sft-10s](https://huggingface.co/kandinskylab/Kandinsky-5.0-T2V-Pro-sft-10s) |
| 3 | k5-pro-i2v-5s-sd | kandinsky5pro_i2v_sft_5s.safetensors | I2V, 5s, 19B, Pro SFT | [kandinskylab/Kandinsky-5.0-I2V-Pro-sft-5s](https://huggingface.co/kandinskylab/Kandinsky-5.0-I2V-Pro-sft-5s) |
| 4 | k5-pro-t2v-5s-sd | kandinsky5pro_t2v_pretrain_5s.safetensors | T2V, 5s, 19B, Pro Pretrain | [kandinskylab/Kandinsky-5.0-T2V-Pro-pretrain-5s](https://huggingface.co/kandinskylab/Kandinsky-5.0-T2V-Pro-pretrain-5s) |
| 5 | k5-pro-t2v-10s-sd | kandinsky5pro_t2v_pretrain_10s.safetensors | T2V, 10s, 19B, Pro Pretrain | [kandinskylab/Kandinsky-5.0-T2V-Pro-pretrain-10s](https://huggingface.co/kandinskylab/Kandinsky-5.0-T2V-Pro-pretrain-10s) |

[Kandinsky 5.0 Video Lite models](https://huggingface.co/collections/kandinskylab/kandinsky-50-video-lite) are technically supported, but were not extensively tested. Community feedback is welcome.

[Kandinsky 5.0 Image Lite models](https://huggingface.co/collections/kandinskylab/kandinsky-50-image-lite) are also supported but not extensively tested.

<details>
<summary>æ—¥æœ¬èª</summary>

`--task` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã‚¿ã‚¹ã‚¯è¨­å®šï¼ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€attentionã€è§£åƒåº¦ã€å„ç¨®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰ã‚’é¸æŠã—ã¾ã™ã€‚
DiTã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ `--dit` ã§æ˜ç¤ºçš„ã«æŒ‡å®šã§ãã¾ã™ï¼ˆã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‘ã‚¹ã‚’ä¸Šæ›¸ãã—ã¾ã™ï¼‰ã€‚

[Kandinsky 5.0 Video Liteãƒ¢ãƒ‡ãƒ«](https://huggingface.co/collections/kandinskylab/kandinsky-50-video-lite) ã¯æŠ€è¡“çš„ã«ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ãŒã€ååˆ†ãªå‹•ä½œç¢ºèªã¯ã§ãã¦ã„ã¾ã›ã‚“ã€‚å•é¡ŒãŒã‚ã‚Œã°ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚

[Kandinsky 5.0 Image Lite ãƒ¢ãƒ‡ãƒ«](https://huggingface.co/collections/kandinskylab/kandinsky-50-image-lite) ã‚‚ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ãŒã€ååˆ†ã«ãƒ†ã‚¹ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚

</details>

## Pre-caching / äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥

Pre-caching is required before training. This involves caching both latents and text encoder outputs. Note that caches created for Video Pro and Lite are NOT interchangeable with ones created for Image Lite - attempting to do this will create errors so please remake the cache when switching between image model/video model training e.g. Flux and Hunyuan VAE types.

### Notes for Kandinsky5 / Kandinsky5ã®æ³¨æ„ç‚¹

- You must cache **text encoder outputs** with `kandinsky5_cache_text_encoder_outputs.py` before training.
- `--text_encoder_qwen` / `--text_encoder_clip` are Hugging Face Transformers models: pass a model ID (recommended) or a local HF snapshot directory.
- For I2V tasks, the latent cache stores both first and last frame latents (`latents_image`, always two frames) when running `kandinsky5_cache_latents.py`â€”one cache works for both first-only and first+last conditioning.
- If you want to train image models (T2I/I2I), you MUST use the Flux VAE and provide `--image_model_training` to `kandinsky5_cache_latents.py`!
- If you want to train image_edit (I2I), you MUST specify `--image_edit_training` to `'kandinsky5_cache_text_encoder_outputs.py` for the text encoder to see the image properly. Do NOT do this for any other mode including T2I or quality will degrade severely.

<details>
<summary>æ—¥æœ¬èª</summary>

ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å‰ã«äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå¿…è¦ã§ã™ã€‚ã“ã‚Œã«ã¯ã€æ½œåœ¨å‡ºåŠ›ã¨ãƒ†ã‚­ã‚¹ãƒˆ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼å‡ºåŠ›ã®ä¸¡æ–¹ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå«ã¾ã‚Œã¾ã™ã€‚ Video Pro ãŠã‚ˆã³ Lite ç”¨ã«ä½œæˆã•ã‚ŒãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ã€Image Lite ç”¨ã«ä½œæˆã•ã‚ŒãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨äº’æ›æ€§ãŒãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã‚’å®Ÿè¡Œã—ã‚ˆã†ã¨ã™ã‚‹ã¨ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ãŸã‚ã€ç”»åƒãƒ¢ãƒ‡ãƒ«ã¨ãƒ“ãƒ‡ã‚ª ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã¨ãã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å†ä½œæˆã—ã¦ãã ã•ã„ã€‚ Flux ãŠã‚ˆã³ Hunyuan VAE ã‚¿ã‚¤ãƒ—ã€‚

- å­¦ç¿’å‰ã«ã€`kandinsky5_cache_text_encoder_outputs.py` ã«ã‚ˆã‚‹ **ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥** ãŒå¿…é ˆã§ã™ã€‚
- `--text_encoder_qwen` / `--text_encoder_clip` ã¯Hugging Face Transformersã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«IDï¼ˆæ¨å¥¨ï¼‰ã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ã®HF snapshotãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚
- I2Vã‚¿ã‚¹ã‚¯ã§ã¯ã€`kandinsky5_cache_latents.py` å®Ÿè¡Œæ™‚ã«æœ€åˆã¨æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ latentï¼ˆ`latents_image`ã€å¸¸ã«2ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰ã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™ã€‚1å›ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§ first / first+last ä¸¡æ–¹ã®ãƒ¢ãƒ¼ãƒ‰ã«å¯¾å¿œã§ãã¾ã™ã€‚
- ç”»åƒãƒ¢ãƒ‡ãƒ« (T2I/I2I) ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å ´åˆã¯ã€Flux VAE ã‚’ä½¿ç”¨ã—ã€`kandinsky5_cache_latents.py` ã« `--image_model_training` ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
- image_edit (I2I) ã‚’å­¦ç¿’ã•ã›ã‚‹å ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒç”»åƒã‚’æ­£ã—ãèªè­˜ã§ãã‚‹ã‚ˆã†ã«ã€`'kandinsky5_cache_text_encoder_outputs.py` ã« `--image_edit_training` ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚T2I ã‚’å«ã‚€ä»–ã®ãƒ¢ãƒ¼ãƒ‰ã§ã¯ã€ã“ã®æ“ä½œã‚’è¡Œã‚ãªã„ã§ãã ã•ã„ã€‚ãã†ã—ãªã„ã¨ã€ç”»è³ªãŒè‘—ã—ãä½ä¸‹ã—ã¾ã™ã€‚

</details>

### Text Encoder Output Pre-caching / ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥

Text encoder output pre-caching is required. Create the cache using the following command:

```bash
python kandinsky5_cache_text_encoder_outputs.py \
    --dataset_config path/to/dataset.toml \
    --text_encoder_qwen Qwen/Qwen2.5-VL-7B-Instruct \
    --text_encoder_auto \
    --text_encoder_clip openai/clip-vit-large-patch14 \
    --batch_size 4
```

Adjust `--batch_size` according to your available VRAM. Add `--image_edit_training` ONLY when training for image edit mode.

For additional options, use `python kandinsky5_cache_text_encoder_outputs.py --help`.

<details>
<summary>æ—¥æœ¬èª</summary>

ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å‡ºåŠ›ã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å¿…é ˆã§ã™ã€‚ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ä½¿ç”¨å¯èƒ½ãªVRAMã«åˆã‚ã›ã¦ `--batch_size` ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚ç”»åƒç·¨é›†ãƒ¢ãƒ¼ãƒ‰ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†å ´åˆã®ã¿ã€`--image_edit_training` ã‚’è¿½åŠ ã—ã¾ã™ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `--help` ã§ç¢ºèªã§ãã¾ã™ã€‚

</details>

### Latent Pre-caching / latentã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥

Latent pre-caching is required. Create the cache using the following command:

```bash
python kandinsky5_cache_latents.py \
    --dataset_config path/to/dataset.toml \
    --vae path/to/vae/diffusion_pytorch_model.safetensors
```

For NABLA training, you may want to build NABLA-compatible latent caches:

```bash
python kandinsky5_cache_latents.py \
    --dataset_config path/to/dataset.toml \
    --vae path/to/vae/diffusion_pytorch_model.safetensors \
    --nabla_resize
```

If you're running low on VRAM, lower the `--batch_size`. If you want to train T2I/I2I, you MUST specify `--image_model_training` here! For image_edit (I2I) training, the `control_images` in the dataset config are used as the reference(ground truth) image. See [Dataset Config](./dataset_config.md#sample-for-image-dataset-with-control-images) for details.

For additional options, use `python kandinsky5_cache_latents.py --help`.

<details>
<summary>æ—¥æœ¬èª</summary>

latentã®äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å¿…é ˆã§ã™ã€‚ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

VRAMãŒè¶³ã‚Šãªã„å ´åˆã¯ã€`--batch_size`ã‚’å°ã•ãã—ã¦ãã ã•ã„ã€‚T2I/I2I ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å ´åˆã¯ã€ã“ã“ã§ã‚‚ `--image_model_training` ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚image_edit (I2I) ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã® `control_images` ãŒå‚ç…§ç”»åƒï¼ˆã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ç”»åƒï¼‰ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚è©³ç´°ã¯ [ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š](./dataset_config.md#sample-for-image-dataset-with-control-images) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

NABLAã§å­¦ç¿’ã™ã‚‹å ´åˆã¯ã€NABLAäº’æ›ã®latentã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ï¼š

```bash
python kandinsky5_cache_latents.py \
    --dataset_config path/to/dataset.toml \
    --vae path/to/vae/diffusion_pytorch_model.safetensors \
    --nabla_resize
```

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `--help` ã§ç¢ºèªã§ãã¾ã™ã€‚

</details>

## Training / å­¦ç¿’

Start training using the following command (input as a single line):

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 \
    kandinsky5_train_network.py \
    --mixed_precision bf16 \
    --dataset_config path/to/dataset.toml \
    --task k5-pro-t2v-5s-sd \
    --dit path/to/kandinsky5pro_t2v_sft_5s.safetensors \
    --text_encoder_qwen Qwen/Qwen2.5-VL-7B-Instruct \
    --text_encoder_clip openai/clip-vit-large-patch14 \
    --vae path/to/vae/diffusion_pytorch_model.safetensors \
    --fp8_base --fp8_scaled \
    --sdpa \
    --gradient_checkpointing \
    --max_data_loader_n_workers 1 \
    --persistent_data_loader_workers \
    --learning_rate 1e-4 \
    --optimizer_type AdamW8Bit \
    --optimizer_args "weight_decay=0.001" "betas=(0.9,0.95)" \
    --max_grad_norm 1.0 \
    --lr_scheduler constant_with_warmup \
    --lr_warmup_steps 100 \
    --network_module networks.lora_kandinsky \
    --network_dim 32 \
    --network_alpha 32 \
    --timestep_sampling shift \
    --discrete_flow_shift 5.0 \
    --output_dir path/to/output/folder \
    --output_name k5_lora \
    --save_every_n_epochs 1 \
    --max_train_epochs 50 
```

For I2V training, switch the task and checkpoint to an I2V preset (e.g., `k5-pro-i2v-5s-sd` with `kandinsky5pro_i2v_sft_5s.safetensors`). The latent cache already stores first and last frame latents (`latents_image`, two frames) when you run `kandinsky5_cache_latents.py`, so the same cache covers both first-only and first+last modesâ€”no extra flags are needed beyond picking an I2V task. For image models (T2I or I2I), make sure to use the Flux VAE and set the appropriate task (`k5-lite-t2i-hd` or `k5-lite-i2i-hd`) here, as well as passing `--image_model_training` to `kandinsky5_cache_latents.py` when caching the latents in the previous step.

**Note on first+last frame conditioning**: First+last frame training support is experimental. The effectiveness and plausibility of this approach have not yet been thoroughly tested. Feedback and results from community testing are welcome.

The training settings are experimental. Appropriate learning rates, training steps, timestep distribution, etc. are not yet fully determined. Feedback is welcome.

For additional options, use `python kandinsky5_train_network.py --help`.

### Key Options / ä¸»è¦ã‚ªãƒ—ã‚·ãƒ§ãƒ³

- `--task`: Model configuration (architecture, attention type, resolution, sampling parameters). See Available Tasks above.
- `--dit`: Path to DiT checkpoint. **Overrides the task's default checkpoint path.** You can use any compatible checkpoint (SFT, pretrain, or your own) with any task config as long as the architecture matches.
- `--vae`: Path to VAE checkpoint (overrides task default)
- `--network_module`: Use `networks.lora_kandinsky` for Kandinsky5 LoRA

**Note**: The `--task` option only sets the model architecture and parameters, not the weights. Use `--dit` to specify which checkpoint to load.

**æ³¨æ„**: `--task`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚’è¨­å®šã—ã€é‡ã¿ã¯è¨­å®šã—ã¾ã›ã‚“ã€‚`--dit`ã§èª­ã¿è¾¼ã‚€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

### Memory Optimization / ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–

`--gradient_checkpointing` enables gradient checkpointing to reduce VRAM usage.

`--fp8_base / --fp8_scaled` runs DiT in fp8 mode. This can significantly reduce memory consumption but may impact output quality.

If you're running low on VRAM, use `--blocks_to_swap` to offload some blocks to CPU. If you OOM on encoding prompts or caching for TE, try `--text_encoder_auto` or `--text_encoder_cpu` to run part or all of the Qwen TE on CPU.

`--gradient_checkpointing_cpu_offload` can be used to offload activations to CPU when using gradient checkpointing. This must be used together with `--gradient_checkpointing`.

### Attention / ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³

Use `--sdpa`, `--flash_attn`, `--flash3`, `--sage_attn`, or `--xformers` to control the attention backend for Kandinsky5.

### Kandinsky5-specific Options / Kandinsky5å›ºæœ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³

- `text_encoder_auto`: Use device_map='auto' for Qwen TE to avoid OOM issues.
- `--i` / `--image`: Init image path for i2v-style seeding in `kandinsky5_generate_video.py`.

**NABLA attention (training):**

- `--use_nabla_attention`: Use NABLA attention.
- `--nabla_method`: NABLA binarization method (default `topcdf`).
- `--nabla_P`: CDF threshold (default `0.9`).
- `--nabla_wT`, `--nabla_wH`, `--nabla_wW`: STA window sizes (defaults `11`, `3`, `3`).
- `--nabla_add_sta` / `--no_nabla_add_sta`: Enable/disable STA prior when forcing NABLA.

**NABLA-compatible latent caching:**

- `kandinsky5_cache_latents.py --nabla_resize`: Resizes inputs to the next multiple of 128 before VAE encoding, which helps produce latents compatible with NABLA geometry constraints.

### Sample Generation During Training / å­¦ç¿’ä¸­ã®ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ

Sample generation during training is supported. See [sampling during training](./sampling_during_training.md) for details.

<details>
<summary>æ—¥æœ¬èª</summary>

ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ï¼ˆå®Ÿéš›ã«ã¯ä¸€è¡Œã§å…¥åŠ›ï¼‰ã€‚

æ—¥æœ¬èªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ä¾‹ï¼ˆè‹±èªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨åŒã˜å†…å®¹ï¼‰ï¼š

```bash
accelerate launch --num_cpu_threads_per_process 1 --mixed_precision bf16 \
    kandinsky5_train_network.py \
    --mixed_precision bf16 \
    --dataset_config path/to/dataset.toml \
    --task k5-pro-t2v-5s-sd \
    --dit path/to/kandinsky5pro_t2v_pretrain_5s.safetensors \
    --text_encoder_qwen Qwen/Qwen2.5-VL-7B-Instruct \
    --text_encoder_clip openai/clip-vit-large-patch14 \
    --vae path/to/vae/diffusion_pytorch_model.safetensors \
    --fp8_base --fp8_scaled \
    --sdpa \
    --gradient_checkpointing \
    --max_data_loader_n_workers 1 \
    --persistent_data_loader_workers \
    --learning_rate 1e-4 \
    --optimizer_type AdamW8Bit \
    --optimizer_args "weight_decay=0.001" "betas=(0.9,0.95)" \
    --max_grad_norm 1.0 \
    --lr_scheduler constant_with_warmup \
    --lr_warmup_steps 100 \
    --network_module networks.lora_kandinsky \
    --network_dim 32 \
    --network_alpha 32 \
    --timestep_sampling shift \
    --discrete_flow_shift 5.0 \
    --output_dir path/to/output \
    --output_name k5_lora \
    --save_every_n_epochs 1 \
    --max_train_epochs 50
```

I2Vã®å­¦ç¿’ã‚’è¡Œã†å ´åˆã¯ã€ã‚¿ã‚¹ã‚¯ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’I2Vå‘ã‘ãƒ—ãƒªã‚»ãƒƒãƒˆã«å¤‰æ›´ã—ã¦ãã ã•ã„ï¼ˆä¾‹: `k5-pro-i2v-5s-sd` ã¨ `kandinsky5pro_i2v_sft_5s.safetensors`ï¼‰ã€‚`kandinsky5_cache_latents.py` ã§latentã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹éš›ã«ã€æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ latentï¼ˆ`latents_image`ï¼‰ã‚‚ä¿å­˜ã•ã‚Œã‚‹ãŸã‚ã€I2Vå°‚ç”¨ã®è¿½åŠ ãƒ•ãƒ©ã‚°ã¯ä¸è¦ã§ã™ï¼ˆI2Vã‚¿ã‚¹ã‚¯ã‚’é¸ã¶ã ã‘ã§å‹•ä½œã—ã¾ã™ï¼‰ã€‚ç”»åƒãƒ¢ãƒ‡ãƒ« (T2I ã¾ãŸã¯ I2I) ã®å ´åˆã¯ã€å¿…ãš Flux VAE ã‚’ä½¿ç”¨ã—ã¦é©åˆ‡ãªã‚¿ã‚¹ã‚¯ (`k5-lite-t2i-hd` ã¾ãŸã¯ `k5_lite_i2i_hd`) ã‚’è¨­å®šã—ã€å‰ã®æ‰‹é †ã§æ½œåœ¨å¤‰æ•°ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã¨ãã« `--image_model_training` ã‚’ `kandinsky5_cache_latents.py` ã«æ¸¡ã™ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚

**æœ€åˆã¨æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ æ¡ä»¶ä»˜ã‘ã«ã¤ã„ã¦**: æœ€åˆã¨æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ å­¦ç¿’ã‚µãƒãƒ¼ãƒˆã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æœ‰åŠ¹æ€§ã¨å¦¥å½“æ€§ã¯ã¾ã ååˆ†ã«ãƒ†ã‚¹ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¨çµæœã‚’ãŠå¾…ã¡ã—ã¦ã„ã¾ã™ã€‚

å­¦ç¿’è¨­å®šã¯å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚é©åˆ‡ãªå­¦ç¿’ç‡ã€å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°ã€ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®åˆ†å¸ƒãªã©ã¯ã€ã¾ã å®Œå…¨ã«ã¯æ±ºã¾ã£ã¦ã„ã¾ã›ã‚“ã€‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ãŠå¾…ã¡ã—ã¦ã„ã¾ã™ã€‚

ãã®ä»–ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ `--help` ã§ç¢ºèªã§ãã¾ã™ã€‚

**ä¸»è¦ã‚ªãƒ—ã‚·ãƒ§ãƒ³**

- `--task`: ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆä¸Šè¨˜ã®åˆ©ç”¨å¯èƒ½ãªã‚¿ã‚¹ã‚¯ã‚’å‚ç…§ï¼‰
- `--dit`: DiTãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¸ã®ãƒ‘ã‚¹ï¼ˆã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä¸Šæ›¸ãï¼‰
- `--vae`: VAEãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¸ã®ãƒ‘ã‚¹ï¼ˆã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä¸Šæ›¸ãï¼‰
- `--network_module`: Kandinsky5 LoRAã«ã¯ `networks.lora_kandinsky` ã‚’ä½¿ç”¨

**ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–**

`--gradient_checkpointing`ã§gradient checkpointingã‚’æœ‰åŠ¹ã«ã—ã€VRAMä½¿ç”¨é‡ã‚’å‰Šæ¸›ã§ãã¾ã™ã€‚

`--fp8_base / --fp8_scaled`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€DiTãŒfp8ã§å­¦ç¿’ã•ã‚Œã¾ã™ã€‚æ¶ˆè²»ãƒ¡ãƒ¢ãƒªã‚’å¤§ããå‰Šæ¸›ã§ãã¾ã™ãŒã€å“è³ªã¯ä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

VRAMãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã€`--blocks_to_swap` ã‚’ä½¿ç”¨ã—ã¦ä¸€éƒ¨ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’ CPU ã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„ TE ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§ãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å ´åˆã¯ã€`--text_encoder_auto` ã¾ãŸã¯ `--text_encoder_cpu` ã‚’ä½¿ç”¨ã—ã¦ã€Qwen TE ã®ä¸€éƒ¨ã¾ãŸã¯ã™ã¹ã¦ã‚’ CPU ã§å®Ÿè¡Œã—ã¦ã¿ã¦ãã ã•ã„ã€‚

`--gradient_checkpointing_cpu_offload`ã‚’æŒ‡å®šã™ã‚‹ã¨ã€gradient checkpointingä½¿ç”¨æ™‚ã«ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚’CPUã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚`--gradient_checkpointing`ã¨ä½µç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

**ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³**

`--sdpa`/`--flash_attn`/`--flash3`/`--sage_attn`/`--xformers`ã¯Kandinsky5ã®attention backendã«é©ç”¨ã•ã‚Œã¾ã™ã€‚

**Kandinsky5å›ºæœ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³**

- `text_encoder_auto`: OOM ã®å•é¡Œã‚’å›é¿ã™ã‚‹ã«ã¯ã€Qwen TE ã« device_map='auto' ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
- `--i` / `--image`: `kandinsky5_generate_video.py` ã§i2vé¢¨ã®åˆæœŸç”»åƒï¼ˆ1ãƒ•ãƒ¬ãƒ¼ãƒ ç›®ã®ã‚·ãƒ¼ãƒ‰ï¼‰ã‚’æŒ‡å®šã—ã¾ã™ã€‚

**NABLAã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆå­¦ç¿’ï¼‰**

- `--use_nabla_attention`: ã‚¿ã‚¹ã‚¯è¨­å®šã«é–¢ä¿‚ãªãNABLAã‚’å¼·åˆ¶ã—ã¾ã™ã€‚
- `--nabla_method`: NABLAã®äºŒå€¤åŒ–ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ `topcdf`ï¼‰ã€‚
- `--nabla_P`: CDFã—ãã„å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ `0.9`ï¼‰ã€‚
- `--nabla_wT`, `--nabla_wH`, `--nabla_wW`: STAã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ `11`, `3`, `3`ï¼‰ã€‚
- `--nabla_add_sta` / `--no_nabla_add_sta`: STA priorã®æœ‰åŠ¹/ç„¡åŠ¹ã€‚

**NABLAäº’æ›latentã‚­ãƒ£ãƒƒã‚·ãƒ¥**

- `kandinsky5_cache_latents.py --nabla_resize`: VAEã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å‰ã«å…¥åŠ›ã‚’128ã®å€æ•°ã¸ãƒªã‚µã‚¤ã‚ºã—ã€NABLAã®å¹¾ä½•æ¡ä»¶ã«åˆã†latentã‚’ç”Ÿæˆã—ã‚„ã™ãã—ã¾ã™ã€‚

**å­¦ç¿’ä¸­ã®ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ**

å­¦ç¿’ä¸­ã®ã‚µãƒ³ãƒ—ãƒ«ç”ŸæˆãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚è©³ç´°ã¯[å­¦ç¿’ä¸­ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°](./sampling_during_training.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

</details>

## Inference / æ¨è«–

Generate videos using the following command:

```bash
python kandinsky5_generate_video.py \
    --task k5-pro-t2v-5s-sd \
    --dit path/to/kandinsky5pro_t2v_sft_5s.safetensors \
    --vae path/to/vae/diffusion_pytorch_model.safetensors \
    --text_encoder_qwen Qwen/Qwen2.5-VL-7B-Instruct \
    --text_encoder_auto \
    --text_encoder_clip openai/clip-vit-large-patch14 \
    --fp8_scaled \
    --dtype bfloat16 \
    --prompt "A cat walks on the grass, realistic style." \
    --negative_prompt "low quality, artifacts" \
    --video_length 121 \
    --steps 50 \
    --guidance_scale 5 \
    --scheduler_scale 10 \
    --seed 42 \
    --width 512 \
    --height 512 \
    --save_path path/to/output/folder/ \
    --lora_weight path/to/lora.safetensors \
    --lora_multiplier 1.0
```

### Options / ã‚ªãƒ—ã‚·ãƒ§ãƒ³

- `--task`: Model configuration
- `--prompt`: Text prompt for generation
- `--negative_prompt`: Negative prompt (optional)
- `--save_path`: Output folder path
- `--width`, `--height`: Output resolution (defaults from task config). I2VI may override this if `--advanced_i2v` not specified!
- `--video_length`: Number of video frames to generate (exclusive of `--frames`)
- `--frames`: Number of latent frames to generate (exclusive of `--video_length`)
- `--steps`: Number of inference steps (defaults from task config)
- `--guidance_scale`: Guidance scale (defaults from task config)
- `--seed`: Random seed, can be an integer or a string! Yep, really!
- `--fp8_scaled`: Use fp8 scaled quantization to reduce size of DiT and save memory/VRAM
- `--fp8_fast`: Use fast fp8 math available on RTX 40X0 (Ada Lovelace) and potentially later GPUs to improve speed substantially for a small quality loss
- `--fp16_fast`: Use optimized fp16 math and fp16 accumulation available in PyTorch 2.7 or later to improve speed substantially. Quality loss is small for Video Pro but may be noticeable for Video Lite and Image!
- `--text_encoder_auto`: Auto split the text encoder between GPU and CPU. Use this if you OOM when encoding prompts!
- `--advanced_i2v`: Eases restrictions on size/shape for I2V/I2I modes and automatically scales input image to requested video size but pushing the model too far outside what it expects can cause issues so use smartly!
- `--blocks_to_swap`: Number of blocks to offload to CPU
- `--lora_weight`: Path(s) to LoRA weight file(s)
- `--lora_multiplier`: LoRA multiplier(s)
- `--optimized`: Overrides the default values of several command line args to provide an optimized but quality experience. Enables fp16_fast or fp8_fast depending on mode and hardware, fp8_scaled, sageattn and torch.compile. Requires SageAttention and Triton to be installed in addition to PyTorch 2.7.0 or higher!
- `--preview_latent_every`: If specified, enables previews (saved to output folder as latent_preview.mp4/png) of the current generation every N steps. By default uses latent2RGB (very fast, lower quality) but can optionally use `--preview_vae` to specify a [TinyAutoencoder](https://huggingface.co/Blyss/BlissfulModels/tree/main/taehv) for fast, high quality previews! Use taehv for Video Pro/Lite and taef1 for Image!

Additional tasks such as Lite and Image tasks are also available as well as various speed optimizations. For a complete list of available flags, please see `python kandinsky5_generate_video.py --help`.

<details>
<summary>æ—¥æœ¬èª</summary>

ä¸Šã®ã‚³ãƒãƒ³ãƒ‰ä¾‹ã‚’ä½¿ç”¨ã—ã¦å‹•ç”»ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

**ã‚ªãƒ—ã‚·ãƒ§ãƒ³**

- `--task`: ãƒ¢ãƒ‡ãƒ«è¨­å®š
- `--prompt`: ç”Ÿæˆç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
- `--negative_prompt`: ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- `--save_path`: å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹
- `--width`, `--height`: å‡ºåŠ›è§£åƒåº¦ï¼ˆã‚¿ã‚¹ã‚¯è¨­å®šã‹ã‚‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ã€‚`--advanced_i2v` ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€I2VI ã¯ã“ã‚Œã‚’ä¸Šæ›¸ãã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
- `--video_length`: ç”Ÿæˆã™ã‚‹ãƒ“ãƒ‡ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼ˆ`--frames` ã‚’é™¤ãï¼‰
- `--frames`: ç”Ÿæˆã™ã‚‹æ½œåœ¨ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼ˆ`--video_length` ã‚’é™¤ãï¼‰
- `--steps`: æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆã‚¿ã‚¹ã‚¯è¨­å®šã‹ã‚‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- `--guidance_scale`: ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆã‚¿ã‚¹ã‚¯è¨­å®šã‹ã‚‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- `--seed`: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã€‚æ•´æ•°ã¾ãŸã¯æ–‡å­—åˆ—ã‚’æŒ‡å®šã§ãã¾ã™ã€‚ã¯ã„ã€æœ¬å½“ã«ãã†ã§ã™ï¼
- `--fp8_scaled`: fp8ã‚¹ã‚±ãƒ¼ãƒ«ã®é‡å­åŒ–ã‚’ä½¿ç”¨ã—ã¦DiTã®ã‚µã‚¤ã‚ºã‚’ç¸®å°ã—ã€ãƒ¡ãƒ¢ãƒª/VRAMã‚’ç¯€ç´„ã—ã¾ã™
- `--fp8_fast`: RTX 40X0 (Ada Lovelace) ãŠã‚ˆã³ãã‚Œä»¥é™ã® GPU ã§åˆ©ç”¨å¯èƒ½ãªé«˜é€Ÿ fp8 æ¼”ç®—ã‚’ä½¿ç”¨ã—ã¦ã€ã‚ãšã‹ãªå“è³ªæå¤±ã§é€Ÿåº¦ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã¾ã™
- `--fp16_fast`: PyTorch 2.7 ä»¥é™ã§åˆ©ç”¨å¯èƒ½ãªæœ€é©åŒ–ã•ã‚ŒãŸ fp16 æ¼”ç®—ãŠã‚ˆã³ fp16 ç´¯ç®—ã‚’ä½¿ç”¨ã—ã¦ã€é€Ÿåº¦ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã¾ã™ã€‚ Video Pro ã§ã¯å“è³ªã®ä½ä¸‹ã¯ã‚ãšã‹ã§ã™ãŒã€Video Lite ã¨ Image ã§ã¯é¡•è‘—ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
- `--text_encoder_auto`: ãƒ†ã‚­ã‚¹ãƒˆ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ GPU ã¨ CPU ã®é–“ã§è‡ªå‹•åˆ†å‰²ã—ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã¨ãã« OOM ã™ã‚‹å ´åˆã¯ã€ã“ã‚Œã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
- `--advanced_i2v`: I2V/I2I ãƒ¢ãƒ¼ãƒ‰ã®ã‚µã‚¤ã‚º/å½¢çŠ¶ã®åˆ¶é™ã‚’ç·©å’Œã—ã€å…¥åŠ›ç”»åƒã‚’è¦æ±‚ã•ã‚ŒãŸãƒ“ãƒ‡ã‚ª ã‚µã‚¤ã‚ºã«è‡ªå‹•çš„ã«ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¾ã™ãŒã€ãƒ¢ãƒ‡ãƒ«ã‚’æœŸå¾…å€¤ã‹ã‚‰å¤§ããå¤–ã—ã™ãã‚‹ã¨å•é¡ŒãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€è³¢ãä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
- `--blocks_to_swap`: CPUã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯æ•°
- `--lora_weight`: LoRAé‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹
- `--lora_multiplier`: LoRAä¿‚æ•°
- `--optimized`: ã„ãã¤ã‹ã®ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã—ã€æœ€é©åŒ–ã•ã‚ŒãŸé«˜å“è³ªãªã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚ãƒ¢ãƒ¼ãƒ‰ã¨ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«å¿œã˜ã¦ fp16_fast ã¾ãŸã¯ fp8_fastã€fp8_scaledã€sageattnã€torch.compile ã‚’æœ‰åŠ¹ã«ã—ã¾ã™ã€‚PyTorch 2.7.0 ä»¥é™ã«åŠ ãˆã¦ã€SageAttention ã¨ Triton ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
- `--preview_latent_every`: æŒ‡å®šã™ã‚‹ã¨ã€ç¾åœ¨ã®ä¸–ä»£ã®Nã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ã«latent_preview.mp4/pngã¨ã—ã¦ä¿å­˜ï¼‰ãŒæœ‰åŠ¹ã«ãªã‚Šã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯latent2RGBï¼ˆéå¸¸ã«é«˜é€Ÿã€ä½å“è³ªï¼‰ã‚’ä½¿ç”¨ã—ã¾ã™ãŒã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§`--preview_vae`ã‚’ä½¿ç”¨ã—ã¦[TinyAutoencoder](https://huggingface.co/Blyss/BlissfulModels/tree/main/taehv)ã‚’æŒ‡å®šã—ã€é«˜é€Ÿã§é«˜å“è³ªã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å®Ÿç¾ã§ãã¾ã™ã€‚Video Pro/Liteã®å ´åˆã¯taehvã€Imageã®å ´åˆã¯taef1ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

Liteã‚¿ã‚¹ã‚¯ã‚„Imageã‚¿ã‚¹ã‚¯ãªã©ã®è¿½åŠ ã‚¿ã‚¹ã‚¯ã«åŠ ãˆã€æ§˜ã€…ãªé€Ÿåº¦æœ€é©åŒ–ã‚‚åˆ©ç”¨å¯èƒ½ã§ã™ã€‚åˆ©ç”¨å¯èƒ½ãªãƒ•ãƒ©ã‚°ã®å®Œå…¨ãªãƒªã‚¹ãƒˆã«ã¤ã„ã¦ã¯ã€`python kandinsky5_generate_video.py --help` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

</details>

## Dataset Configuration / ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š

Dataset configuration is the same as other architectures. See [dataset configuration](./dataset_config.md) for details.

<details>
<summary>æ—¥æœ¬èª</summary>

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã¯ä»–ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨åŒã˜ã§ã™ã€‚è©³ç´°ã¯[ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š](./dataset_config.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

</details>
